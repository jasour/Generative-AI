# Generative-AI
Generative AI Foundations: Algorithms and Architectures

“Generative AI Foundations: Algorithms and Architectures” is a comprehensive and technically rigorous guide to modern generative modeling techniques. It systematically covers fundamental principles, key algorithms (such as flow models, diffusion models, VAEs, GANs, and autoregressive models), and the underlying neural network architectures (such as CNNs, U-Nets, Transformers, and multimodal architectures) that enable state-of-the-art generative AI systems. The course balances mathematical depth with conceptual clarity, providing precise formulations of training objectives, optimization methods, and sampling strategies.
Complete with references to seminal research papers and emerging methodologies, this work serves as a valuable learning resource for advanced students, researchers, and practitioners aiming to master the foundations and frontiers of generative AI.
It extends the discussion to real-world applications, notably robotic trajectory generation using diffusion models, illustrating how generative AI principles can be applied beyond image and text generation. 


**Topics:**

**Generative AI - Algorithms:**

1. Flow Models

2. ODE-based Flow Models - Flow Matching

3. SDE-based Denoising Diffusion Models - Score Matching

4. Denoising Diffusion Models

5. Autoencoder & Variational Autoencoder (VAE)

6. Latent-Space Diffusion Model

7. Autoregressive Models

8. Generative Adversarial Networks (GANs)

**Generative AI - Architectures:**

1. Multilayer Perceptron (MLP)
2. Training, Different Loss functions
3. Backpropagation algorithm, Stochastic Gradient Descent (SGD), Adam Optimizer
4. Common Training Issues and Regularization in Deep Learning
5. Scaling Laws
6. Gen AI Input/Output/Layer Types
7. Convolutional Neural Network (CNN)
8. PixelCNN
9. U-Net Denoising Model
10. Recurrent Neural Networks (RNN)
11. LSTM (Long Short-Term Memory)
12. GRU (Gated Recurrent Unit)
13. Transformers
14. Multi-Head Attention, Self Attention, Cross Attention
15. Diffusion Transformers (DiT)
16. Multimodal Models


**Appendices:**

1. Key Differential Equations in Generative AI
2. Fine-tuning Large Language Models
3. Deep Reinforcement Learning - Key Concepts and Summary
	– PG, VPG, PPO, DDPG, TD3, SAC
4. Reinforcement Learning from Human Feedback (RLHF) and Imitation Learning
5. Adversarial Training, Robustness in Language Models, and Language Models Evaluation
6. Python Libraries for Generative AI 





